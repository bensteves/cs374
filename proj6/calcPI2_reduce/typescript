Script started on 2021-10-29 21:04:17-04:00 [TERM="xterm-256color" TTY="/dev/pts/0" COLUMNS="80" LINES="24"]
]0;bjs48@gold28: ~/cs374/proj6/calcPI2_reduce[01;32mbjs48@gold28[00m:[01;34m~/cs374/proj6/calcPI2_reduce[00m$ cat pthrea      calcPI2.c
/* calcPI2.c calculates PI using POSIX threads.
 * Since PI == 4 * arctan(1), and arctan(x) is the 
 *  integral from 0 to x of (1/(1+x*x),
 *  the for loop below approximates that integration.
 *
 * Joel Adams, Calvin College, Fall 2013.
 * 
 * Edits by: Ben Steves
 * CS 374
 * 10-28-21
 *
 * Usage: ./calcPI2 [numIntervals] [numThreads]
 */

#include <stdio.h>                 // printf(), fprintf(), etc.
#include <stdlib.h>                // strtoul(), exit(), ...
#include <pthread.h>               // pthreads
#include <mpi.h>                   // MPI_Wtime()
#include <math.h>
#include "pthreadReductionSum.h"

// global variables (shared by all threads 
volatile long double pi = 0.0;       // our approximation of PI 
pthread_mutex_t      piLock;         // how we synchronize writes to 'pi' 
long double          intervals = 0;  // how finely we chop up the integration 
unsigned long        numThreads = 0; // how many threads we use 

/* compute PI using the parallel for loop pattern
 * Parameters: arg, a void* 
 * Preconditions: 
 *   - non-locals intervals and numThreads are defined.
 *   - arg contains the address of our thread's ID.
 * Postcondition: non-local pi contains our approximation of PI.
 */
void * computePI(void * arg)
{
    long double   x,
                  width,
                  localSum = 0;
    unsigned long i,
                  threadID = *((unsigned long *)arg);

    width = 1.0 / intervals;
    
    

    for(i = threadID ; i < intervals; i += numThreads) {
        x = (i + 0.5) * width;
        localSum += 4.0 / (1.0 + x*x);
    }
    
    localSum *= width; 

    pthreadReductionSum(localSum, &pi, threadID, numThreads);
	
    /* pthread_mutex_lock(&piLock);
    pi += localSum;
    pthread_mutex_unlock(&piLock); */ 

    return NULL;
} 




/* process the command-line arguments
 * Parameters: argc, an int; and argv a char**.
 * Postcondition:
 *  - non-locals intervals and numThreads have been defined.
 *     according to the values the user specified when
 *     calcPI2 was invoked.
 */
void processCommandLine(int argc, char ** argv) {
   if ( argc == 3 ) {
      intervals = strtoul(argv[1], 0, 10); 
      numThreads = strtoul(argv[2], 0, 10); 
   } else if ( argc == 2 ) {
      intervals = strtoul(argv[1], 0, 10);
      numThreads = 1;
   } else if ( argc == 1 ) {
      intervals = 1;
      numThreads = 1;
   } else {
      fprintf(stderr, "\nUsage: calcPI2 [intervals] [numThreads]\n\n");
      exit(1);
   }
}
      

int main(int argc, char **argv) {
    pthread_t * threads;            // dynamic array of threads 
    unsigned long  * threadID;      // dynamic array of thread id #s 
    unsigned long i;                // loop control variable 
    double startTime = 0,           // timing variables
           stopTime = 0;
    processCommandLine(argc, argv);

    threads = malloc(numThreads*sizeof(pthread_t));
    threadID = malloc(numThreads*sizeof(unsigned long));
    localSumArray = malloc((numThreads*2)*sizeof(long double)); //allocates a little more memory than needed
    
    
    
   pthread_mutex_init(&piLock, NULL);

    MPI_Init(&argc, &argv);
    startTime = MPI_Wtime();
    

    for (i = 0; i < numThreads; i++) {   // fork threads
        threadID[i] = i;
        pthread_create(&threads[i], NULL, computePI, threadID+i);
    }

    for (i = 0; i < numThreads; i++) {   // join them
        pthread_join(threads[i], NULL);
    }
    stopTime = MPI_Wtime();
    

    printf("Estimation of pi is %32.30Lf in %lf secs\n", pi, stopTime - startTime);
    printf("(actual pi value is 3.141592653589793238462643383279...)\n");
    
   /* used for debugging
    
    for ( int i = 0; i < 16; i++) { printf("%Lf \n", localSumArray[i]); }
    
   */
   
    pthread_mutex_destroy(&piLock);
    free(threads);
    free(threadID);
    free(localSumArray);
    MPI_Finalize();
    return 0;
}

]0;bjs48@gold28: ~/cs374/proj6/calcPI2_reduce[01;32mbjs48@gold28[00m:[01;34m~/cs374/proj6/calcPI2_reduce[00m$ cat pthreadReductionSum.h
/* pthreadReductionSum.h performs reduction pattern
 * to speed up mutex and to calculate pi
 *
 * Contains pthreadReductionSum()
 *
 * author    Ben Steves
 * why       Project 6 for CS374
 * where     Calvin University
 * date      10-28-21
 */
 
#include "pthreadBarrier.h"
#include <stdio.h>
#include <pthread.h>
#include <stdlib.h>
#include <math.h>

//array to store sums for all threads
long double * localSumArray;

/* pthreadReductionSum uses reduction pattern to sum to some number
* (in this instance pi)
*
* written by - Ben Steves
* other sources - linked below
* https://sodocumentation.net/cuda/topic/6566/parallel-reduction--e-g--how-to-sum-an-array-
*
* parameter: localVal (type long double) - a value computed by one thread
* parameter: finalVal (type long double) - the final value to come out of reduction
* parameter: threadID (type unsigned long) - what thread is doing the work
* parameter: numThreads (type unsigned long) - number of threads
*
* postcondition: pi is calculated with the reduction pattern
*/
void pthreadReductionSum(long double localVal, volatile long double * finalVal, 
			unsigned long threadID, unsigned long numThreads) {	
	
	//initialize what our stride will be										
	int stride;

	//stride is always 2^x, so our divisions in stride can always be by two
	    int x = 0;
	    while (pow(2, x) < numThreads) {
	    	x++;
	    }
	    stride = pow(2, x) / 2;
	//}


	/* if the number of threads != stride, thread 0 inserts 0's dynamically
	* and treat extra 0's as values during reduction
	* (for numThreads where dividing by 2 each stride fails.. ex 6 or 10)
	*/
	if (threadID == 0) {
	    localSumArray = malloc((numThreads*2)*sizeof(long double)); 
	    for (int i = numThreads; i < (stride*2); i++) {
    		localSumArray[i] = 0.0;
       	    }
	}
	
	//put computed values into the array
	pthreadBarrier(numThreads);
       	localSumArray[threadID] = localVal;
	
	//reduction pattern, which works for any number of processes
	for (int i = stride; i > 0; i /= 2) {
		
		pthreadBarrier(numThreads); 
		if (threadID < i) {
			localSumArray[threadID] += localSumArray[threadID + i];
		}
		//pthreadBarrier(numThreads); 
		
	}
	
	//clean out our barrier and assign reduced value to the finalVal
	barrierCleanup();
	if (threadID == 0) {
		*finalVal = localSumArray[0];
	}
}
]0;bjs48@gold28: ~/cs374/proj6/calcPI2_reduce[01;32mbjs48@gold28[00m:[01;34m~/cs374/proj6/calcPI2_reduce[00m$ mpirun ./calcPI2 1000000000 4
Estimation of pi is 3.141592653589793591745876755184 in 1.553132 secs
(actual pi value is 3.141592653589793238462643383279...)
]0;bjs48@gold28: ~/cs374/proj6/calcPI2_reduce[01;32mbjs48@gold28[00m:[01;34m~/cs374/proj6/calcPI2_reduce[00m$ exit

Script done on 2021-10-29 21:05:09-04:00 [COMMAND_EXIT_CODE="0"]
